{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\snake\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\snake\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from termcolor import colored\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc , precision_score , classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics.classification import accuracy_score, log_loss\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#from mlxtend.classifier import StackingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.cross_validation import StratifiedKFold \n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import math\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of tou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7269</th>\n",
       "      <td>3343</td>\n",
       "      <td>@mention Google plze Tammi.  I'm in middle of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>5334</td>\n",
       "      <td>RT @mention ÷¼ Are you all set? ÷_ {link} ÷...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7271</th>\n",
       "      <td>5378</td>\n",
       "      <td>RT @mention Aha! Found proof of lactation room...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7272</th>\n",
       "      <td>2173</td>\n",
       "      <td>We just launched our iPad app at #SXSW! Get al...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7273</th>\n",
       "      <td>3162</td>\n",
       "      <td>The next fin serv battle is vs Apple, GOOG, Mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7274 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                              tweet  sentiment\n",
       "0         1701  #sxswnui #sxsw #apple defining language of tou...          1\n",
       "1         1851  Learning ab Google doodles! All doodles should...          1\n",
       "2         2689  one of the most in-your-face ex. of stealing t...          2\n",
       "3         4525  This iPhone #SXSW app would b pretty awesome i...          0\n",
       "4         3604  Line outside the Apple store in Austin waiting...          1\n",
       "...        ...                                                ...        ...\n",
       "7269      3343  @mention Google plze Tammi.  I'm in middle of ...          1\n",
       "7270      5334  RT @mention ÷¼ Are you all set? ÷_ {link} ÷...          1\n",
       "7271      5378  RT @mention Aha! Found proof of lactation room...          1\n",
       "7272      2173  We just launched our iPad app at #SXSW! Get al...          1\n",
       "7273      3162  The next fin serv battle is vs Apple, GOOG, Mo...          1\n",
       "\n",
       "[7274 rows x 3 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tweet'] = data['tweet'].apply(lambda row: str(row))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x = x.replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"don't\", \"do not\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\";\", \" \").replace(\"&\", \" \")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'] = data['tweet'].fillna(\"\").apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mProcessing train data\u001b[0m\n",
      "\u001b[33mRemoving user handles starting with @\u001b[0m\n",
      "\u001b[33mRemoving numbers and special characters\u001b[0m\n",
      "\u001b[33mRemoving urls\u001b[0m\n",
      "\u001b[33mRemoving single characters\u001b[0m\n",
      "\u001b[33mTokenizing\u001b[0m\n",
      "\u001b[33mRemoving stopwords\u001b[0m\n",
      "\u001b[33mExpanding not words\u001b[0m\n",
      "\u001b[33mLemmatizing the words\u001b[0m\n",
      "\u001b[33mStemming the words\u001b[0m\n",
      "\u001b[33mCombining words back to tweets\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Setting stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STOPWORDS.remove(\"not\")\n",
    "STOPWORDS.remove('no')\n",
    "STOPWORDS.remove('against')\n",
    "STOPWORDS.remove('because')\n",
    "STOPWORDS.remove('while')\n",
    "STOPWORDS.remove('why')\n",
    "STOPWORDS.remove('how')\n",
    "STOPWORDS.remove('can')\n",
    "STOPWORDS.add('quot')\n",
    "STOPWORDS.add('RT')\n",
    "STOPWORDS.add('sxsw')\n",
    "STOPWORDS.add('SXSW')\n",
    "STOPWORDS.add('link')\n",
    "STOPWORDS.add('The')\n",
    "STOPWORDS.add('Apple')\n",
    "STOPWORDS.add('Google')\n",
    "STOPWORDS.add('iPad')\n",
    "# Function to expand tweet\n",
    "def expand_tweet(tweet):\n",
    "    expanded_tweet = []\n",
    "    for word in tweet:\n",
    "        if re.search(\"n't\", word):\n",
    "            expanded_tweet.append(word.split(\"n't\")[0])\n",
    "            expanded_tweet.append(\"not\")\n",
    "        else:\n",
    "            expanded_tweet.append(word)\n",
    "    return expanded_tweet\n",
    "\n",
    "# Function to process tweets\n",
    "def clean_tweet(data, wordNetLemmatizer, porterStemmer):\n",
    "    data['Clean_tweet'] = data['tweet']\n",
    "    print(colored(\"Removing user handles starting with @\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].str.replace(\"@[\\w]*\",\"\")\n",
    "    print(colored(\"Removing numbers and special characters\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].str.replace(\"[^a-zA-Z' ]\",\"\")\n",
    "    print(colored(\"Removing urls\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(r\"((www\\.[^\\s]+)|(https?://[^\\s]+))\"), \"\")\n",
    "    print(colored(\"Removing single characters\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(r\"(^| ).( |$)\"), \" \")\n",
    "    print(colored(\"Tokenizing\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].str.split()\n",
    "    print(colored(\"Removing stopwords\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [word for word in tweet if word not in STOPWORDS])\n",
    "    print(colored(\"Expanding not words\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: expand_tweet(tweet))\n",
    "    print(colored(\"Lemmatizing the words\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [wordNetLemmatizer.lemmatize(word) for word in tweet])\n",
    "    print(colored(\"Stemming the words\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [porterStemmer.stem(word) for word in tweet])\n",
    "    print(colored(\"Combining words back to tweets\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: ' '.join(tweet))\n",
    "    return data\n",
    "\n",
    "# Define processing methods\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "porterStemmer = PorterStemmer()\n",
    "\n",
    "# Pre-processing the tweets\n",
    "print(colored(\"Processing train data\", \"green\"))\n",
    "train_data = clean_tweet(data, wordNetLemmatizer, porterStemmer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of tou...</td>\n",
       "      <td>1</td>\n",
       "      <td>sxswnui appl defin languag touch differ dialec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should...</td>\n",
       "      <td>1</td>\n",
       "      <td>learn ab doodl all doodl light funni amp innov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing t...</td>\n",
       "      <td>2</td>\n",
       "      <td>one inyourfac ex steal show yr At school mkt e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome i...</td>\n",
       "      <td>0</td>\n",
       "      <td>thi iphon app would pretti awesom not crash ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting...</td>\n",
       "      <td>1</td>\n",
       "      <td>line outsid store austin wait new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7269</th>\n",
       "      <td>3343</td>\n",
       "      <td>@mention Google plze Tammi.  I'm in middle of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>plze tammi i'm middl crazi everyth soooooo busi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>5334</td>\n",
       "      <td>RT @mention ÷¼ Are you all set? ÷_ {link} ÷...</td>\n",
       "      <td>1</td>\n",
       "      <td>are set edchat musedchat sxswi newtwitt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7271</th>\n",
       "      <td>5378</td>\n",
       "      <td>RT @mention Aha! Found proof of lactation room...</td>\n",
       "      <td>1</td>\n",
       "      <td>aha found proof lactat room excus mother' room...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7272</th>\n",
       "      <td>2173</td>\n",
       "      <td>We just launched our iPad app at #SXSW! Get al...</td>\n",
       "      <td>1</td>\n",
       "      <td>We launch app get detail first edit free</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7273</th>\n",
       "      <td>3162</td>\n",
       "      <td>The next fin serv battle is vs Apple, GOOG, Mo...</td>\n",
       "      <td>1</td>\n",
       "      <td>next fin serv battl v goog mobil oper they con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7274 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                              tweet  sentiment  \\\n",
       "0         1701  #sxswnui #sxsw #apple defining language of tou...          1   \n",
       "1         1851  Learning ab Google doodles! All doodles should...          1   \n",
       "2         2689  one of the most in-your-face ex. of stealing t...          2   \n",
       "3         4525  This iPhone #SXSW app would b pretty awesome i...          0   \n",
       "4         3604  Line outside the Apple store in Austin waiting...          1   \n",
       "...        ...                                                ...        ...   \n",
       "7269      3343  @mention Google plze Tammi.  I'm in middle of ...          1   \n",
       "7270      5334  RT @mention ÷¼ Are you all set? ÷_ {link} ÷...          1   \n",
       "7271      5378  RT @mention Aha! Found proof of lactation room...          1   \n",
       "7272      2173  We just launched our iPad app at #SXSW! Get al...          1   \n",
       "7273      3162  The next fin serv battle is vs Apple, GOOG, Mo...          1   \n",
       "\n",
       "                                            Clean_tweet  \n",
       "0     sxswnui appl defin languag touch differ dialec...  \n",
       "1     learn ab doodl all doodl light funni amp innov...  \n",
       "2     one inyourfac ex steal show yr At school mkt e...  \n",
       "3     thi iphon app would pretti awesom not crash ev...  \n",
       "4                     line outsid store austin wait new  \n",
       "...                                                 ...  \n",
       "7269    plze tammi i'm middl crazi everyth soooooo busi  \n",
       "7270            are set edchat musedchat sxswi newtwitt  \n",
       "7271  aha found proof lactat room excus mother' room...  \n",
       "7272           We launch app get detail first edit free  \n",
       "7273  next fin serv battl v goog mobil oper they con...  \n",
       "\n",
       "[7274 rows x 4 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer( ngram_range=(1,2))\n",
    "kk = tfidf.fit_transform(train_data.Clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7274x32695 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 59596 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = SGDClassifier(alpha=0.0001, penalty='l2', loss='log',)\n",
    "# clf.fit(kk,train_data.sentiment)\n",
    "# sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n",
    "# sig_clf.fit(kk,train_data.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sig_clf = LogisticRegression(C=1000)\n",
    "sig_clf.fit(kk,train_data.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7506</td>\n",
       "      <td>Audience Q: What prototyping tools do you use?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7992</td>\n",
       "      <td>At SXSW? Send Your Best Photos &amp;amp; Videos to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>247</td>\n",
       "      <td>@mention  and here's a pic of you winning your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7688</td>\n",
       "      <td>Google Marissa Mayer: mobile phone as a cursor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3294</td>\n",
       "      <td>#SXSW Google maps is even cooler than I thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>1550</td>\n",
       "      <td>@mention @mention @mention Hmmm....how fast ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>1933</td>\n",
       "      <td>Samsung Galaxy S II Appears At FCC And Team An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>9052</td>\n",
       "      <td>@mention You could buy a new iPad 2 tmrw at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>4219</td>\n",
       "      <td>Wow very long queue of people at apple pop up ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>7210</td>\n",
       "      <td>Privacy Could Headline Google Circles Social N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1819 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                              tweet\n",
       "0         7506  Audience Q: What prototyping tools do you use?...\n",
       "1         7992  At SXSW? Send Your Best Photos &amp; Videos to...\n",
       "2          247  @mention  and here's a pic of you winning your...\n",
       "3         7688  Google Marissa Mayer: mobile phone as a cursor...\n",
       "4         3294    #SXSW Google maps is even cooler than I thought\n",
       "...        ...                                                ...\n",
       "1814      1550  @mention @mention @mention Hmmm....how fast ca...\n",
       "1815      1933  Samsung Galaxy S II Appears At FCC And Team An...\n",
       "1816      9052  @mention You could buy a new iPad 2 tmrw at th...\n",
       "1817      4219  Wow very long queue of people at apple pop up ...\n",
       "1818      7210  Privacy Could Headline Google Circles Social N...\n",
       "\n",
       "[1819 rows x 2 columns]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata = pd.read_csv('test.csv')\n",
    "testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x = x.replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"don't\", \"do not\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\";\", \" \").replace(\"&\", \" \")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['tweet'] = test_data['tweet'].fillna(\"\").apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mProcessing train data\u001b[0m\n",
      "\u001b[33mRemoving user handles starting with @\u001b[0m\n",
      "\u001b[33mRemoving numbers and special characters\u001b[0m\n",
      "\u001b[33mRemoving urls\u001b[0m\n",
      "\u001b[33mRemoving single characters\u001b[0m\n",
      "\u001b[33mTokenizing\u001b[0m\n",
      "\u001b[33mRemoving stopwords\u001b[0m\n",
      "\u001b[33mExpanding not words\u001b[0m\n",
      "\u001b[33mLemmatizing the words\u001b[0m\n",
      "\u001b[33mStemming the words\u001b[0m\n",
      "\u001b[33mCombining words back to tweets\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Setting stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STOPWORDS.remove(\"not\")\n",
    "STOPWORDS.remove('no')\n",
    "STOPWORDS.remove('against')\n",
    "STOPWORDS.remove('because')\n",
    "STOPWORDS.remove('while')\n",
    "STOPWORDS.remove('why')\n",
    "STOPWORDS.remove('how')\n",
    "STOPWORDS.remove('can')\n",
    "STOPWORDS.add('quot')\n",
    "STOPWORDS.add('RT')\n",
    "STOPWORDS.add('sxsw')\n",
    "STOPWORDS.add('SXSW')\n",
    "STOPWORDS.add('link')\n",
    "STOPWORDS.add('The')\n",
    "STOPWORDS.add('Apple')\n",
    "STOPWORDS.add('Google')\n",
    "STOPWORDS.add('iPad')\n",
    "\n",
    "# Function to expand tweet\n",
    "def expand_tweet(tweet):\n",
    "    expanded_tweet = []\n",
    "    for word in tweet:\n",
    "        if re.search(\"n't\", word):\n",
    "            expanded_tweet.append(word.split(\"n't\")[0])\n",
    "            expanded_tweet.append(\"not\")\n",
    "        else:\n",
    "            expanded_tweet.append(word)\n",
    "    return expanded_tweet\n",
    "\n",
    "# Function to process tweets\n",
    "def clean_tweet(data, wordNetLemmatizer, porterStemmer):\n",
    "    data['Clean_tweet'] = testdata['tweet']\n",
    "    print(colored(\"Removing user handles starting with @\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].str.replace(\"@[\\w]*\",\"\")\n",
    "    print(colored(\"Removing numbers and special characters\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].str.replace(\"[^a-zA-Z' ]\",\"\")\n",
    "    print(colored(\"Removing urls\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(r\"((www\\.[^\\s]+)|(https?://[^\\s]+))\"), \"\")\n",
    "    print(colored(\"Removing single characters\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(r\"(^| ).( |$)\"), \" \")\n",
    "    print(colored(\"Tokenizing\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].str.split()\n",
    "    print(colored(\"Removing stopwords\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [word for word in tweet if word not in STOPWORDS])\n",
    "    print(colored(\"Expanding not words\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: expand_tweet(tweet))\n",
    "    print(colored(\"Lemmatizing the words\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [wordNetLemmatizer.lemmatize(word) for word in tweet])\n",
    "    print(colored(\"Stemming the words\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [porterStemmer.stem(word) for word in tweet])\n",
    "    print(colored(\"Combining words back to tweets\", \"yellow\"))\n",
    "    data['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: ' '.join(tweet))\n",
    "    return data\n",
    "\n",
    "# Define processing methods\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "porterStemmer = PorterStemmer()\n",
    "\n",
    "# Pre-processing the tweets\n",
    "print(colored(\"Processing train data\", \"green\"))\n",
    "test_data = clean_tweet(testdata, wordNetLemmatizer, porterStemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7506</td>\n",
       "      <td>Audience Q: What prototyping tools do you use?...</td>\n",
       "      <td>audienc what prototyp tool use sketchbooksshar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7992</td>\n",
       "      <td>At SXSW? Send Your Best Photos &amp;amp; Videos to...</td>\n",
       "      <td>At send your best photo amp video citizenjourn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>247</td>\n",
       "      <td>@mention  and here's a pic of you winning your...</td>\n",
       "      <td>here' pic win ipad unsix cc cont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7688</td>\n",
       "      <td>Google Marissa Mayer: mobile phone as a cursor...</td>\n",
       "      <td>marissa mayer mobil phone cursor physic locat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3294</td>\n",
       "      <td>#SXSW Google maps is even cooler than I thought</td>\n",
       "      <td>map even cooler thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>1550</td>\n",
       "      <td>@mention @mention @mention Hmmm....how fast ca...</td>\n",
       "      <td>hmmmhow fast can appl build new store time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>1933</td>\n",
       "      <td>Samsung Galaxy S II Appears At FCC And Team An...</td>\n",
       "      <td>samsung galaxi II appear At fcc and team andro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>9052</td>\n",
       "      <td>@mention You could buy a new iPad 2 tmrw at th...</td>\n",
       "      <td>you could buy new tmrw popup store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>4219</td>\n",
       "      <td>Wow very long queue of people at apple pop up ...</td>\n",
       "      <td>wow long queue peopl appl pop store bought ipa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>7210</td>\n",
       "      <td>Privacy Could Headline Google Circles Social N...</td>\n",
       "      <td>privaci could headlin circl social network rev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1819 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id                                              tweet  \\\n",
       "0         7506  Audience Q: What prototyping tools do you use?...   \n",
       "1         7992  At SXSW? Send Your Best Photos &amp; Videos to...   \n",
       "2          247  @mention  and here's a pic of you winning your...   \n",
       "3         7688  Google Marissa Mayer: mobile phone as a cursor...   \n",
       "4         3294    #SXSW Google maps is even cooler than I thought   \n",
       "...        ...                                                ...   \n",
       "1814      1550  @mention @mention @mention Hmmm....how fast ca...   \n",
       "1815      1933  Samsung Galaxy S II Appears At FCC And Team An...   \n",
       "1816      9052  @mention You could buy a new iPad 2 tmrw at th...   \n",
       "1817      4219  Wow very long queue of people at apple pop up ...   \n",
       "1818      7210  Privacy Could Headline Google Circles Social N...   \n",
       "\n",
       "                                            Clean_tweet  \n",
       "0     audienc what prototyp tool use sketchbooksshar...  \n",
       "1     At send your best photo amp video citizenjourn...  \n",
       "2                      here' pic win ipad unsix cc cont  \n",
       "3     marissa mayer mobil phone cursor physic locat ...  \n",
       "4                               map even cooler thought  \n",
       "...                                                 ...  \n",
       "1814         hmmmhow fast can appl build new store time  \n",
       "1815  samsung galaxi II appear At fcc and team andro...  \n",
       "1816                 you could buy new tmrw popup store  \n",
       "1817  wow long queue peopl appl pop store bought ipa...  \n",
       "1818  privaci could headlin circl social network rev...  \n",
       "\n",
       "[1819 rows x 3 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = test_data.Clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1819x32695 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8744 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf = tfidf.transform(test_tfidf)\n",
    "test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest= sig_clf.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = pd.DataFrame(ytest,columns=['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>247</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7688</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>1550</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>1933</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>9052</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>4219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>7210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1819 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id  sentiment\n",
       "0         7506          1\n",
       "1         7992          1\n",
       "2          247          1\n",
       "3         7688          2\n",
       "4         3294          1\n",
       "...        ...        ...\n",
       "1814      1550          1\n",
       "1815      1933          1\n",
       "1816      9052          1\n",
       "1817      4219          1\n",
       "1818      7210          1\n",
       "\n",
       "[1819 rows x 2 columns]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.concat([test_data['tweet_id'],pd.Series(ytest)],axis=1)\n",
    "result.columns = ['tweet_id','sentiment']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('final1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
